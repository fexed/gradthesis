@misc{disarlidatasetpreproc,
    author = "Daniele Di Sarli",
    title = "RNN benchmarking multi-layer",
    url  = "https://colab.research.google.com/drive/1ko5kUsAkI4bDhW9EKH7WescO9rjJ71vp",
    addendum = "(accessed: 06.08.2021)",
    keywords = "python,dataset,preprocessing"
}

@inproceedings{teaching2020,
  title={TEACHING-Trustworthy autonomous cyber-physical applications through human-centred intelligence},
  author={Bacciu, Davide and Akarmazyan, Siranush and Armengaud, Eric and Bacco, Manlio and Bravos, George and Calandra, Calogero and Carlini, Emanuele and Carta, Antonio and Cassar{\`a}, Pietro and Coppola, Massimo and others},
  booktitle={2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)},
  pages={1--6},
  year={2021},
  organization={IEEE}

@misc{lomonaco_2019,
    title={Continual learning for production systems}, url={https://medium.com/continual-ai/continual-learning-for-production-systems-304cc9f60603},
    journal={Medium},
    publisher={ContinualAI},
    author={Lomonaco, Vincenzo},
    year={2019},
    month={Aug}
} 

@misc{moriarty_ewc,
    title={Continual Learning with Elastic Weight Consolidation in TensorFlow 2}, url={https://seanmoriarity.com/2020/10/18/continual-learning-with-ewc/},
    author={Moriarty, Sean},
    year={2020},
    month={October},
    addendum = "(accessed: 10.07.2021)",
    keywords = "python,ewc,code"
}

@inproceedings{10.1145/3242969.3242985,
author = {Schmidt, Philip and Reiss, Attila and Duerichen, Robert and Marberger, Claus and Van Laerhoven, Kristof},
title = {Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3242985},
doi = {10.1145/3242969.3242985},
abstract = {Affect recognition aims to detect a person's affective state based on observables,
with the goal to e.g. improve human-computer interaction. Long-term stress is known
to have severe implications on wellbeing, which call for continuous and automated
stress monitoring systems. However, the affective computing community lacks commonly
used standard datasets for wearable stress detection which a) provide multimodal high-quality
data, and b) include multiple affective states. Therefore, we introduce WESAD, a new
publicly available dataset for wearable stress and affect detection. This multimodal
dataset features physiological and motion data, recorded from both a wrist- and a
chest-worn device, of 15 subjects during a lab study. The following sensor modalities
are included: blood volume pulse, electrocardiogram, electrodermal activity, electromyogram,
respiration, body temperature, and three-axis acceleration. Moreover, the dataset
bridges the gap between previous lab studies on stress and emotions, by containing
three different affective states (neutral, stress, amusement). In addition, self-reports
of the subjects, which were obtained using several established questionnaires, are
contained in the dataset. Furthermore, a benchmark is created on the dataset, using
well-known features and standard machine learning methods. Considering the three-class
classification problem ( baseline vs. stress vs. amusement ), we achieved classification
accuracies of up to 80%,. In the binary case ( stress vs. non-stress ), accuracies
of up to 93%, were reached. Finally, we provide a detailed analysis and comparison
of the two device locations ( chest vs. wrist ) as well as the different sensor modalities.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {400–408},
numpages = {9},
keywords = {multimodal dataset, sensor fusion, benchmark, user study, emotion recognition, affective computing, stress detection},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@INPROCEEDINGS{6246152,  author={Reiss, Attila and Stricker, Didier},  booktitle={2012 16th International Symposium on Wearable Computers},   title={Introducing a New Benchmarked Dataset for Activity Monitoring},   year={2012},  volume={},  number={},  pages={108-109},  doi={10.1109/ISWC.2012.13}}

@INPROCEEDINGS{5573462,  author={Roggen, Daniel and Calatroni, Alberto and Rossi, Mirco and Holleczek, Thomas and Förster, Kilian and Tröster, Gerhard and Lukowicz, Paul and Bannach, David and Pirkl, Gerald and Ferscha, Alois and Doppler, Jakob and Holzmann, Clemens and Kurz, Marc and Holl, Gerald and Chavarriaga, Ricardo and Sagha, Hesam and Bayati, Hamidreza and Creatura, Marco and Millàn, José del R.},  booktitle={2010 Seventh International Conference on Networked Sensing Systems (INSS)},   title={Collecting complex activity datasets in highly rich networked sensor environments},   year={2010},  volume={},  number={},  pages={233-240},  doi={10.1109/INSS.2010.5573462}}

@inproceedings{10.1145/2809695.2809718,
author = {Stisen, Allan and Blunck, Henrik and Bhattacharya, Sourav and Prentow, Thor Siiger and Kj\ae{}rgaard, Mikkel Baun and Dey, Anind and Sonne, Tobias and Jensen, Mads M\o{}ller},
title = {Smart Devices Are Different: Assessing and MitigatingMobile Sensing Heterogeneities for Activity Recognition},
year = {2015},
isbn = {9781450336314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809695.2809718},
doi = {10.1145/2809695.2809718},
abstract = {The widespread presence of motion sensors on users' personal mobile devices has spawned
a growing research interest in human activity recognition (HAR). However, when deployed
at a large-scale, e.g., on multiple devices, the performance of a HAR system is often
significantly lower than in reported research results. This is due to variations in
training and test device hardware and their operating system characteristics among
others. In this paper, we systematically investigate sensor-, device- and workload-specific
heterogeneities using 36 smartphones and smartwatches, consisting of 13 different
device models from four manufacturers. Furthermore, we conduct experiments with nine
users and investigate popular feature representation and classification techniques
in HAR research. Our results indicate that on-device sensor and sensor handling heterogeneities
impair HAR performances significantly. Moreover, the impairments vary significantly
across devices and depends on the type of recognition technique used. We systematically
evaluate the effect of mobile sensing heterogeneities on HAR and propose a novel clustering-based
mitigation technique suitable for large-scale deployment of HAR, where heterogeneity
of devices and their usage scenarios are intrinsic.},
booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
pages = {127–140},
numpages = {14},
keywords = {mobile sensing, activity recognition},
location = {Seoul, South Korea},
series = {SenSys '15}
}

@ARTICLE{7736040,
  author={Subramanian, Ramanathan and Wache, Julia and Abadi, Mojtaba Khomami and Vieriu, Radu L. and Winkler, Stefan and Sebe, Nicu},
  journal={IEEE Transactions on Affective Computing}, 
  title={ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors}, 
  year={2018},
  volume={9},
  number={2},
  pages={147-160},
  doi={10.1109/TAFFC.2016.2625250}}

@article{li2017learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@misc{adampaper,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kirkpatrick2017overcoming,
      title={Overcoming catastrophic forgetting in neural networks}, 
      author={James Kirkpatrick and Razvan Pascanu and Neil Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
      year={2017},
      eprint={1612.00796},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/Lopez-PazR17,
  author    = {David Lopez{-}Paz and
               Marc'Aurelio Ranzato},
  title     = {Gradient Episodic Memory for Continuum Learning},
  journal   = {CoRR},
  volume    = {abs/1706.08840},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.08840},
  archivePrefix = {arXiv},
  eprint    = {1706.08840},
  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lopez-PazR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v15-glorot11a,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {
http://proceedings.mlr.press/v15/glorot11a.html
},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@incollection{MCCLOSKEY1989109,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen},
abstract = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.}
}

@TECHREPORT{Riedmiller92rprop-,
    author = {Martin Riedmiller and Heinrich Braun},
    title = {RPROP - A Fast Adaptive Learning Algorithm},
    institution = {Proc. of ISCIS VII), Universitat},
    year = {1992}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{diethe2019continual,
  title={Continual learning in practice},
  author={Diethe, Tom and Borchert, Tom and Thereska, Eno and Balle, Borja and Lawrence, Neil},
  journal={arXiv preprint arXiv:1903.05202},
  year={2019}
}

@article{Jha_2021,
   title={Continual learning in sensor-based human activity recognition: An empirical benchmark analysis},
   volume={575},
   ISSN={0020-0255},
   url={http://dx.doi.org/10.1016/j.ins.2021.04.062},
   DOI={10.1016/j.ins.2021.04.062},
   journal={Information Sciences},
   publisher={Elsevier BV},
   author={Jha, Saurav and Schiemer, Martin and Zambonelli, Franco and Ye, Juan},
   year={2021},
   month={Oct},
   pages={1–21}
}

@misc{priyanshu2021continual,
      title={Continual Distributed Learning for Crisis Management}, 
      author={Aman Priyanshu and Mudit Sinha and Shreyans Mehta},
      year={2021},
      eprint={2104.12876},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agilemanifesto,
    author = "Kent Beck; James Grenning; Robert C. Martin; Mike Beedle; Jim Highsmith; Steve Mellor; Arie van Bennekum; Andrew Hunt; Ken Schwaber; Alistair Cockburn; Ron Jeffries; Jeff Sutherland; Ward Cunningham; Jon Kern; Dave Thomas; Martin Fowler; Brian Marick",
    title = "Manifesto for Agile Software Development",
    url  = "http://agilemanifesto.org/",
    addendum = "(accessed: 06.08.2021)",
    keywords = "project"
}

@misc{vandeven2019scenarios,
      title={Three scenarios for continual learning}, 
      author={Gido M. van de Ven and Andreas S. Tolias},
      year={2019},
      eprint={1904.07734},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{vandeven2019generative,
      title={Generative replay with feedback connections as a general strategy for continual learning}, 
      author={Gido M. van de Ven and Andreas S. Tolias},
      year={2019},
      eprint={1809.10635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rolnick2019experience,
      title={Experience Replay for Continual Learning}, 
      author={David Rolnick and Arun Ahuja and Jonathan Schwarz and Timothy P. Lillicrap and Greg Wayne},
      year={2019},
      eprint={1811.11682},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Carranza_Garc_a_2021,
   title={Enhancing object detection for autonomous driving by optimizing anchor generation and addressing class imbalance},
   volume={449},
   ISSN={0925-2312},
   url={http://dx.doi.org/10.1016/j.neucom.2021.04.001},
   DOI={10.1016/j.neucom.2021.04.001},
   journal={Neurocomputing},
   publisher={Elsevier BV},
   author={Carranza-García, Manuel and Lara-Benítez, Pedro and García-Gutiérrez, Jorge and Riquelme, José C.},
   year={2021},
   month={Aug},
   pages={229–244}
}

@article{huang2020autonomous,
  title={Autonomous driving with deep learning: A survey of state-of-art technologies},
  author={Huang, Yu and Chen, Yue},
  journal={arXiv preprint arXiv:2006.06091},
  year={2020}
}

@INPROCEEDINGS{6856582,
  author={Wei, Junqing and Snider, Jarrod M. and Gu, Tianyu and Dolan, John M. and Litkouhi, Bakhtiar},
  booktitle={2014 IEEE Intelligent Vehicles Symposium Proceedings}, 
  title={A behavioral planning framework for autonomous driving}, 
  year={2014},
  volume={},
  number={},
  pages={458-464},
  doi={10.1109/IVS.2014.6856582}}
  
@misc{abadi2016tensorflow,
      title={TensorFlow: A system for large-scale machine learning}, 
      author={Martín Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
      year={2016},
      eprint={1605.08695},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@book{geron,
    title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition},
    author = {Aurélien Géron},
    isbn = {9781492032649},
    year = {2019},
    publisher = {O'Reilly Media, Inc.},
    keywords = {machinelearning, tensorflow, python}
}

@book{chollet,
    title = {Deep Learning with Python},
    author = {François Chollet},
    isbn = {9781617294433},
    year = {2017},
    publisher = {Manning Publications},
    keywords = {machinelearning, python}
}

@inproceedings{10.1145/2818346.2820736,
author = {Wache, Julia and Subramanian, Ramanathan and Abadi, Mojtaba Khomami and Vieriu, Radu-Laurentiu and Sebe, Nicu and Winkler, Stefan},
title = {Implicit User-Centric Personality Recognition Based on Physiological Responses to Emotional Videos},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2820736},
doi = {10.1145/2818346.2820736},
abstract = {We present a novel framework for recognizing personality traits based on users' physiological
responses to affective movie clips. Extending studies that have correlated explicit/implicit
affective user responses with Extraversion and Neuroticism traits, we perform single-trial
recognition of the big-five traits from Electrocardiogram (ECG), Galvanic Skin Response
(GSR), Electroencephalogram (EEG) and facial emotional responses compiled from 36
users using off-the-shelf sensors. Firstly, we examine relationships among personality
scales and (explicit) affective user ratings acquired in the context of prior observations.
Secondly, we isolate physiological correlates of personality traits. Finally, unimodal
and multimodal personality recognition results are presented. Personality differences
are better revealed while analyzing responses to emotionally homogeneous (e.g., high
valence, high arousal) clips, and significantly above-chance recognition is achieved
for all five traits.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {239–246},
numpages = {8},
keywords = {affective physiological responses, measurement, human factors, verification, personality recognition, algorithms},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@article{Ring2004CHILDAF,
  title={CHILD: A First Step Towards Continual Learning},
  author={Mark B. Ring},
  journal={Machine Learning},
  year={2004},
  volume={28},
  pages={77-104}
}

@inproceedings{0471349119,
  title={FEEDFORWARD NEURAL NETWORKS : AN INTRODUCTION},
  addendum = "(accessed: 19.09.2021)",
  author={Simon Haykin},
  year={2004}
}

@article{rosenblatt1958perceptron,
  added-at = {2017-07-19T15:29:59.000+0200},
  author = {Rosenblatt, F.},
  biburl = {https://www.bibsonomy.org/bibtex/214ee8da21c66cd4d00d7ab6eca2d96a9/andreashdez},
  citeulike-article-id = {13697582},
  citeulike-linkout-0 = {http://dx.doi.org/10.1037/h0042519},
  doi = {10.1037/h0042519},
  interhash = {dc0cef9dc06033a04f525efdcde7a660},
  intrahash = {14ee8da21c66cd4d00d7ab6eca2d96a9},
  issn = {0033-295X},
  journal = {Psychological Review},
  keywords = {imported},
  number = 6,
  pages = {386--408},
  posted-at = {2016-05-02 20:23:36},
  priority = {2},
  timestamp = {2017-07-19T15:31:02.000+0200},
  title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
  url = {http://dx.doi.org/10.1037/h0042519},
  volume = 65,
  year = 1958
}

@article{10.1162/neco.1997.9.8.1735,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@article{DBLP:journals/corr/abs-2003-05991,
  author    = {Dor Bank and
               Noam Koenigstein and
               Raja Giryes},
  title     = {Autoencoders},
  journal   = {CoRR},
  volume    = {abs/2003.05991},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.05991},
  eprinttype = {arXiv},
  eprint    = {2003.05991},
  timestamp = {Tue, 17 Mar 2020 14:18:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-05991.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2011-12960,
  author    = {Lars Lien Ankile and
               Morgan Feet Heggland and
               Kjartan Krange},
  title     = {Deep Convolutional Neural Networks: {A} survey of the foundations,
               selected improvements, and some current applications},
  journal   = {CoRR},
  volume    = {abs/2011.12960},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.12960},
  eprinttype = {arXiv},
  eprint    = {2011.12960},
  timestamp = {Tue, 01 Dec 2020 14:59:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-12960.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{GALLICCHIO201833,
title = {Design of deep echo state networks},
journal = {Neural Networks},
volume = {108},
pages = {33-47},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302223},
author = {Claudio Gallicchio and Alessio Micheli and Luca Pedrelli},
keywords = {Reservoir computing, Echo state networks, Deep echo state networks, Deep recurrent neural networks, Architectural design of recurrent neural networks},
abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.}
}

@article{Rumelhart_1986,
	doi = {10.1038/323533a0},
	url = {https://doi.org/10.1038%2F323533a0},
	year = 1986,
	month = {oct},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {323},
	number = {6088},
	pages = {533--536},
	author = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
	title = {Learning representations by back-propagating errors},
	journal = {Nature}
}

@article{DBLP:journals/corr/abs-1211-5063,
  author    = {Razvan Pascanu and
               Tom{\'{a}}s Mikolov and
               Yoshua Bengio},
  title     = {Understanding the exploding gradient problem},
  journal   = {CoRR},
  volume    = {abs/1211.5063},
  year      = {2012},
  url       = {http://arxiv.org/abs/1211.5063},
  eprinttype = {arXiv},
  eprint    = {1211.5063},
  timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1211-5063.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/ChoMGBSB14,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  eprinttype = {arXiv},
  eprint    = {1406.1078},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}