\chapter{Approcci}
In questa sezione verranno delineati nei dettagli i vari approcci scelti per la realizzazione della comparazione fra alcune strategie di continual learning triviali applicate ai dataset di Human State Monitoring presentati nel precedente capitolo.

\section{Metriche}
Per poter confrontare fra loro le diverse strategie, fin da subito è apparsa chiara la necessità di metriche che misurassero precisamente alcune caratteristiche fondamentali degli approccio di continual learning: in particolare, l'accuratezza sul test set, l'accuratezza media sulle classi e i \textit{forward} e \textit{backward transfer} per poter misurare l'impatto della nuova conoscenza sulla precedente e su quella da acquisire.
\subsection{Accuratezza}
Con "accuratezza" si intende la misura della percentuale di previsioni corrette che modello di machine learning ottiene su un test set, cioè un insieme di dati a cui il modello non è ancora stato esposto.\\
Per poter misurare l'accuratezza su un insieme di dati, l'API di TensorFlow mette a disposizione un comando che valuta un modello di machine learning applicato ai dati passati come parametro:
\begin{lstlisting}[style = myPython]
model.evaluate(Xtest, ytest)
\end{lstlisting}
Questa chiamata ritorna il valore assunto dalla funzione di loss al termine della valutazione e le metriche che vengono specificate durante la compilazione del modello. Compilando il modello con il seguente comando
\begin{lstlisting}[style = myPython]
model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
\end{lstlisting}
otterremo un modello la cui funzione di loss è la cross-entropia categorica, usata per modelli che devono classificare dati appartenenti ad una sola classe fra diverse disponibili, e come metrica aggiuntiva richiediamo l'accuratezza. Così facendo, con
\begin{lstlisting}[style = myPython]
model.evaluate(Xtest, ytest)[1]
\end{lstlisting}
otteniamo la misura dell'accuratezza che il modello \texttt{model} ottiene sul test set composto dai dati \texttt{Xtest} e dalle label \texttt{ytest}.\\\\
La misura di accuratezza indicata nei risultati è l'accuratezza media ottenuta dal modello durante le varie sessioni di addestramento.
\subsection{ACC, FWT e BWT}
Queste tre metriche sono state tratte e adattate dalla pubblicazione \textit{Gradient Episodic Memory for Continual Learning}$^{\cite{DBLP:journals/corr/Lopez-PazR17}}$, e si basano sulla costruzione di una matrice $R \in \mathbb{R}^{E\times T}$. Con $T$ si indica il numero di classi del problema in esame, e ogni volta che il modello finisce una sessione di addestramento tra le $E$ che vengono eseguite si va a valutare la sua \textit{test performance} su tutte le $T$ classi. Così facendo, su calcolano le $R_{i,j}\in R$ valutazioni dell'accuratezza del test di classificazione del modello sulla classe $t_j$ dopo aver osservato gli esempi della $i$-esima sessione di addestramento. Inoltre, una volta creato il modello con una inizializzazione dei pesi casuale si calcola $\overline{b}$, valutazione delle accuratezza nei test sui vari task.\\
Possiamo ora definire le tre metriche:
\begin{itemize}
    \item[-] \textbf{ACC}, \textit{Accuracy} o accuratezza
        \begin{equation}\label{eq:fun_acc}
            ACC = \frac{1}{T}\cdot\sum_{i=1}^T R_{E,i}
        \end{equation}
    Va a misurare l'accuratezza del modello su tutti le $T$ classi a fine addestramento
    \item[-] \textbf{BWT}, \textit{Backward Transfer} o trasferimento all'indietro
        \begin{equation}\label{eq:fun_bwt}
            BWT = \frac{1}{T-1}\cdot\sum_{i=1}^T\left(\frac{1}{E}\sum_{j=1}^E R_{E,i} - R_{j,i}\right)
        \end{equation}
    Valuta l'influenza che ha una sessione di apprendimento sulle precedenti, valutando l'impatto che ha avuto sulle valutazioni dell'accuratezza in fase di test. Si ha un BWT positivo quando una sessione di addestramento migliora l'accuratezza delle precedenti già valutate, mentre un valore negativo indica che la sessione di addestramento ha deteriorato l'accuratezza che si otteneva grazie alla precedente conoscenza. Non è significativo parlare di BWT durante la prima sessione di addestramento.\\
    Un BWT fortemente negativo si ha nello scenario del \textit{catastrophic forgetting}.
    \item[-] \textbf{FWT}, \textit{Forward Transfer} o trasferimento in avanti
        \begin{equation}\label{eq:fun_fwt}
            FWT = \frac{1}{T-1}\cdot\sum_{i=1}^T\left(\frac{1}{E}\sum_{j=1}^E R_{j,i} - \overline{b}_i\right)
        \end{equation}
    Serve per misurare l'influenza che la sessione di apprendimento ha sulla conoscenza ancora da apprendere. In particolare, un FWT positivo è possibile quando il modello è in grado di realizzare lo \textit{zero-shot} learning, cioè di apprendere informazioni sui task futuri senza essere esposto ad esempi da apprendere, per esempio sfruttando la struttura interna della conoscenza in caso di task simili.\\
    Non è significativo parlare di FWT durante l'ultima sessione di addestramento.
\end{itemize}
Tra queste tre, quella senza dubbio più interessante ai fini dell'esperimento è il BWT. Diventa particolarmente importante evitare gli scenari che portano al \textit{catastrophic forgetting}: nel nostro caso, uno scenario simile porterebbe a dimenticare esempi di HSM passati e a far sì che il modello si basi solamente sull'esperienza recente, rischiando di dimenticare importanti esempi passati di stati d'animo e di conseguenza a non saper più riconoscerli correttamente.\\\\
Più grandi sono queste metriche, migliore è il comportamento del modello in esame. A parità di ACC, si preferisce il modello con maggior BWT e FWT che denoterebbe un miglior trasferimento della conoscenza attraverso i task e le sessioni di training.
\subsection{Altre metriche}
Le altre metriche raccolte durante gli esperimenti sono le seguenti:
\begin{itemize}
    \item[-] \textbf{Numero di epoche} medio, e deviazione standard\\
    Questa misurazione fornisce una indicazione sul tempo di convergenza delle reti neurali selezionate sui dati, così da poter stimare le loro prestazioni su hardware diversi da quello di test.
    \item[-] \textbf{Tempo di addestramento} medio, e deviazione standard\\
    Gli esperimenti sono eseguiti su un comune computer casalingo, sfruttando le ottimizzazioni messe a disposizione dall'utilizzo di una GPU per i calcoli. Nello specifico, le computazioni sono state eseguite su una macchina che montava una GPU nVidia GeForce GTX 1070.\\% CUDA 10.1 https://ai-benchmark.com/ranking_deeplearning.html
    Il tempo di addestramento è calcolato come la media dei tempi impiegati delle varie sessioni di addestramento.
    \item[-] \textbf{Quantità di memoria} media\\
    Metrica utile per valutare l'impatto sulla memoria del sistema che andrà poi a utilizzare il modello addestrato. Un elevato consumo di memoria può portare alla esclusione di dispositivi portatili o con specifiche particolari.
\end{itemize}

\section{Baseline: l'approccio offline}
Un classico addestramento di un modello di machine learning avviene raccogliendo i dati di training e sottoponendoli al modello tutti in una volta. Questo approccio è statico e produce un modello che sui futuri dati si comporterà tanto meglio quanto essi saranno simili ai dati di addestramento.\\
La scelta di usare questo approccio classico come baseline è stata guidata dal principio che, avendo tutti i dati a disposizione in una sola volta, la rete neurale risultante dovrebbe avere le migliori prestazioni possibili sul test set una volta che viene sottoposto, e quindi fornisce un "limite superiore" all'accuratezza degli approcci continual. Una strategia di continual learning è tanto migliore quanto più si avvicina alle performance che la stessa rete neurale avrebbe se addestrata su tutti i dati in maniera offline o, se possibile, quando la supera.\\\\
L'approccio offline quindi avviene seguendo il seguente pseudo-algoritmo, applicabile sia a WESAD che ad ASCERTAIN:
\begin{enumerate}
    \item \textbf{Creare e compilare la rete neurale} che verrà poi addestrata.\\
    La creazione della rete neurale, attraverso la API Sequential di TensorFlow, è analoga per entrambi i dataset a meno della struttura interna. L'esempio seguente è tratto dall'addestramento offline per WESAD, e costruisce una rete neurale con due layer da 18 unità GRU e un layer output da 4 unità, corrispondenti alle 4 classi del dataset.
    \lstinputlisting[style=myPython, firstnumber=22, firstline=22, lastline=30]{code/wesad_totaltrain.py}
    \item \textbf{Caricare il dataset}, composto da training set e test set.\\
    Come precedentemente spiegato, i dataset sono divisi in training set e test set, e il training set è ulteriormente diviso fra i soggetti dei due studi. In entrambi i casi quindi, per quanto riguarda l'approccio offline, viene caricato il test set così com'è, mentre il training set è caricato concantenando fra loro i dati di tutti i soggetti. Esempio, sempre tratto da WESAD:
    \lstinputlisting[style=myPython, firstnumber=35, firstline=35, lastline=47]{code/wesad_totaltrain.py}
    \lstinputlisting[style=myPython, firstnumber=51, firstline=51, lastline=52]{code/wesad_totaltrain.py}
    Il training set è ulteriormente diviso in training set e validation set, utile per verificare l'andamento dell'addestramento e implementare la fermata anticipata dell'addestramento:
    \lstinputlisting[style=myPython, firstnumber=56, firstline=56, lastline=56]{code/wesad_totaltrain.py}
    \item \textbf{Addestramento} e risultati.\\
    Una volta preparato il dataset, si può dare il via all'addestramento. Prima di esso si preparano, sempre grazie all'API di TensorFlow, le due callback usate durante gli esperimenti: la creazione di grafici attraverso TensorBoard e la fermata anticipata in base all'andamento del valore della funzione di loss sul validation set:
    \lstinputlisting[style=myPython, firstnumber=59, firstline=59, lastline=68]{code/wesad_totaltrain.py}
    I risultati sono poi proiettati in output sul terminale:
    \lstinputlisting[style=myPython, firstnumber=70, firstline=70, lastline=73]{code/wesad_totaltrain.py}
\end{enumerate}
L'approccio offline, come visto, è immediato e grazie alle API di TensorFlow anche semplice da implementare. Esso è stato utilizzato per poter selezionare la rete neurale usata per il dataset relativo, una per WESAD e una per ASCERTAIN, su ciascun esperimento, in base all'accuratezza finale raggiunta.

\section{Approcci continual}
Al centro di questo progetto ci sono gli approcci di continual learning. Ne sono stati selezionati alcuni: 3 tecniche di replay e 2 tecniche che non sfruttano esempi già visti.\\\\
Per realizzare il flusso di dati necessario a implementare una strategia di continual learning, le quali richiedono che i dati di addestramento non siano subito tutti disponibili ma che lo diventino nel tempo, i soggetti di entrambi i dataset sono stati divisi a coppie e sottoposti all'addestramento della rete neurale una coppia per volta. Le tabelle \ref{tab:coppiewesad} e \ref{tab:coppieascertain} mostrano la suddivisione a coppie rispettivamente per il dataset WESAD e il dataset ASCERTAIN.
\begin{table}[h]
    \parbox{.45\linewidth}{
    	\begin{center}
    		\begin{tabular}{l|c}
    		     \textbf{Sessione} & \textbf{Coppie}\\
    		     \hline
    		     1 & S2, S3 \\
    		     2 & S4, S5 \\
    		     3 & S6, S7 \\
    		     4 & S8, S9 \\
    		     5 & S10, S11 \\
    		     6 & S13, S14 \\
    		     7 & S15, S16 \\
    		     \hline
    		     $E$ & 7
    		\end{tabular}
    		\caption{Suddivisione a coppie del dataset WESAD}
    		\label{tab:coppiewesad}
    	\end{center}
	}
    \parbox{.45\linewidth}{
    	\begin{center}
    		\begin{tabular}{l|c}
    		     \textbf{Sessione} & \textbf{Coppie}\\
    		     \hline
    		     1 & S0, S1 \\
    		     2 & S2, S3 \\
    		     3 & S4, S5 \\
    		     4 & S6, S7 \\
    		     5 & S8, S9 \\
    		     6 & S10, S11 \\
    		     7 & S12, S13 \\
    		     8 & S14, S15 \\
    		     \hline
    		     $E$ & 8
    		\end{tabular}
    		\caption{Suddivisione a coppie del dataset ASCERTAIN}
    		\label{tab:coppieascertain}
    	\end{center}
    }
\end{table}

Per poter misurare le metriche ACC, BWT e FWT degli approcci di continual learning, in ogni esperimento viene prima preparato il test set suddividendolo nelle varie classi:
\lstinputlisting[style=myPython, firstnumber=21, firstline=21, lastline=22]{code/wesad_continualtrain.py}
\lstinputlisting[style=myPython, firstnumber=24, firstline=24, lastline=44]{code/wesad_continualtrain.py}
\pagebreak
Subito dopo, appena viene creato il modello, vengono inizializzate le variabili necessarie e il vettore $\overline{b}$:
\lstinputlisting[style=myPython, firstnumber=57, firstline=57, lastline=64]{code/wesad_continualtrain.py}
Al termine di ogni sessione di training, cioè appena concluso l'addestramento su una coppia, vengono calcolati gli $R_{i,j}$:
\lstinputlisting[style=myPython, firstnumber=97, firstline=97, lastline=97]{code/wesad_continualtrain.py}
Infine, al termine del processo di addestramento, vengono calcolate le varie metriche:
\lstinputlisting[style=myPython, firstnumber=99, firstline=99, lastline=118]{code/wesad_continualtrain.py}
% Approcci continual
% Dettagli sugli approcci di continual learning adottati per gli esperimenti
% Spiegare nel dettaglio la differenza fra offline e approcci continual, quale sarebbe la situazione ideale (a fine soggetti in continual dovrei avere la stessa accuracy dell'approccio offline) e quale la realtà allo stato dell'arte
% parlare nel dettaglio delle tecniche di continual learning utilizzate, in particolare di LWF e EWC che non sono basate sul semplice replay, facendo riferimento ai paper relativi

\subsection{Continual}
Il primo approccio continual sperimentato è stato definito semplicemente \textit{continual learning}. Questo approccio consiste nel fornire al modello una coppia alla volta come training set, senza ulteriori meccanismi.\\\\
Nel dettaglio, inizialmente vengono subito stabilite le coppie:
\lstinputlisting[style=myPython, firstnumber=66, firstline=66, lastline=66]{code/wesad_continualtrain.py}
Ogni coppia è caricata, concatenata, per formare il training set della sessione che viene poi diviso il training set e validation set:
\lstinputlisting[style=myPython, firstnumber=68, firstline=68, lastline=71]{code/wesad_continualtrain.py}
\lstinputlisting[style=myPython, firstnumber=73, firstline=73, lastline=74]{code/wesad_continualtrain.py}
\lstinputlisting[style=myPython, firstnumber=83, firstline=83, lastline=83]{code/wesad_continualtrain.py}
Infine il modello è addestrato sul training set e validato sul validation set appena prodotti, sono calcolate le metriche e proiettate in output.
\subsection{Replay}  % una percentuale, di solito 25%
Questa è la prima tecnica di replay provata. Consiste nel tenere una percentuale del training set attuale e usarla insieme al training set, cioè alla coppia, della sessione di addestramento successiva. Il procedimento è simile all'approccio continual learning precedente.\\\\
Subito prima di avviare l'addestramento, vengono inizializzate le due variabili che conterranno il training set:
\lstinputlisting[style=myPython, firstnumber=65, firstline=65, lastline=65]{code/wesad_continualreplaytrain.py}
\pagebreak
Appena caricati i soggetti, viene costruito il training set anche mantenendo ciò che è stato trattenuto dalla sessione precedente, se presente:
\lstinputlisting[style=myPython, firstnumber=74, firstline=74, lastline=79]{code/wesad_continualreplaytrain.py}
A fine sessione di addestramento, si trattiene la percentuale di replay (nell'esempio, il 25\%):
\lstinputlisting[style=myPython, firstnumber=100, firstline=100, lastline=100]{code/wesad_continualreplaytrain.py}

\subsection{Cumulative}  % si mantengono gli esempi precedenti nei successivi addestramenti, così all'arrivo dell'ultima coppia di soggetti il training set è uguale al caso offline
Con questo approccio, si intende mantenere di volta in volta l'intero training set. Così facendo, con l'ultima coppia di soggetti si avrà un training set pari all'approccio offline, contenendo tutti i dati di tutti i precedenti soggetti, con cui addestrare la rete neurale già addestrata.\\\\
Viene realizzato semplicemente concatenando la coppia di soggetti al training set usato durante l'ultima sessione di allenamento. Durante la prima sessione di allenamento, viene semplicemente usata la prima coppia di soggetti.
\subsection{Episodic}  % si mantengono di volta in volta m esempi per ogni classe tratti dal training set
Nelle precedenti tecniche di replay si mantengono tutti i dati o un sottoinsieme scelto casualmente del training set. Con questa tecnica, invece, viene mantenuto di sessione in sessione un numero di esempi per ogni etichetta, così da avere un replay bilanciato rispetto alle classi.\\\\
Subito prima di avviare l'addestramento, vengono inizializzate le "memorie episodiche" e il parametro $m$, che indica quanti esempi per classe mantenere (nell'esempio, $m = 70$):
\lstinputlisting[style=myPython, firstnumber=68, firstline=68, lastline=69]{code/wesad_episodic.py}
\pagebreak
Al termine di una sessione di addestramento, il training set appena usato è suddiviso per classi:
\lstinputlisting[style=myPython, firstnumber=105, firstline=105, lastline=108]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=112, firstline=112, lastline=115]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=119, firstline=119, lastline=122]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=126, firstline=126, lastline=129]{code/wesad_episodic.py}
Ogni sottoinsieme è mescolato, per evitare di mantenere sempre i soliti esempi, e da ognuno ne vengono presi $m$:
\lstinputlisting[style=myPython, firstnumber=109, firstline=109, lastline=111]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=116, firstline=116, lastline=118]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=123, firstline=123, lastline=125]{code/wesad_episodic.py}
\lstinputlisting[style=myPython, firstnumber=130, firstline=130, lastline=132]{code/wesad_episodic.py}
Infine, viene formato il training set che andrà a concatenarsi alla coppia di soggetti successiva:
\lstinputlisting[style=myPython, firstnumber=134, firstline=134, lastline=135]{code/wesad_episodic.py}

\subsection{Elastic Weight Consolidation}
Questo approccio di continual learning, introdotto nella pubblicazione \textit{Overcoming catastrophic forgetting in neural networks}$^{\cite{kirkpatrick2017overcoming}}$, si basa sul principio che non tutti i parametri di una rete neurale sono importanti ai fini dell'apprendimento di un determinato task. Dato un modello con parametri $\Theta$ e un dataset $D$, si può formalizzare il problema dell'apprendimento come la ricerca della parametrizzazione $\Theta$ dato il dataset $D$ che massimizza $p(\Theta\:|\:D)$. Seguendo il Teorema di Bayes
\begin{equation}\label{eq:ewc_bayes}
    p(\Theta\:|\:D) = \frac{p(D\:|\:\Theta)\cdot p(\Theta)}{p(D)}
\end{equation}
che può essere trasformata in 
\begin{equation}\label{eq:ewc_bayes2}
    \log p(\Theta\:|\:D) = \log p(D\:|\:\Theta) + \log p(\Theta) - \log p(D)
\end{equation}
Assumendo che il dataset sia diviso in due parti, $D_A$ e $D_B$, poiché l'obiettivo è apprendere $p(\Theta\:|\:D)$ con la suddivisione diventa apprendere prima $p(\Theta\:|\:D_A)$ e successivamente $p(\Theta\:|\:D_B)$. Algebricamente, l'obiettivo è
\begin{equation}\label{eq:ewc2}
    p(\Theta\:|\:D) = p(p(\Theta\:|\:D_A)\:|\:D_b) = \frac{p(D_B\:|\:p(\Theta\:|\:D_A))\cdot p(\Theta\:|\:D_A)}{p(D_B)}
\end{equation}
che, analogamente a \ref{eq:ewc_bayes}, può essere trasformata in
\begin{equation}\label{eq:ewc3}
    \log p(\Theta\:|\:D) = \log p(D_B\:|\:\Theta) + \log p(\Theta\:|\:D_A) - \log p(D_B)
\end{equation}
Si può dedurre come tutta la conoscenza relativa al task $A$ viene assorbita dal termine $p(\theta\:|\: D_A)$, cioè esso indica quali parametri sono importanti per il task $A$. Con $p(\theta\:|\: D_A)$ si può quindi calcolare una matrice $F_i$, chiamata Matrice dell'Informazione di Fisher, per ogni parametro $\Theta_i \in \Theta$ rispetto a $D_A$: questa matrice stima l'importanza del parametro $\Theta_i$ per quanto riguarda il task descritto dai dati $D_A$. Questa informazione può essere usata come "penalità elastica", e nella pubblicazione viene fatta l'analogia ad un elastico che trattiene il valore di $\Theta_i$ dal distanziarsi troppo dalla precedente soluzione, in modo proporzionale a $F_i$. La penalità elastica, cioè il termine di regolarizzazione, è\begin{equation}\label{eq:ewc_penalty}
    \Omega(\Theta) = \sum_i \frac{1}{2} F_i\left(\Theta_i - \Theta_{A,i}^*\right)^2
\end{equation}
che rappresenta l'elasticità con cui ogni parametro $\Theta_i \in \Theta$ viene mantenuto vicino a $\Theta_{A,i}^*\in\Theta_A^*$. Si aggiunge anche un ulteriore parametro, $\lambda$ che descrive l'importanza del termine di regolarizzazione.\\\\
Con queste informazioni, la loss che viene minimizzata nell'approccio EWC è
\begin{equation}\label{eq:ewc_loss}
    loss(\Theta) = loss_B(\Theta) + \sum_i \frac{\lambda}{2} F_i\left(\Theta_i - \Theta_{A,i}^*\right)^2
\end{equation}
dove $loss_B(\Theta)$ è la loss calcolata solamente sul task $B$, $\lambda$ è un parametro che indica l'importanza dei vecchi task rispetto ai nuovi e $i$ etichetta ogni parametro. La matrice $F$ è approssimata come la media dei gradienti delle $\log$-verosomiglianze di $N$ esempi presi da $D_A$ al quadrato:
\begin{equation}\label{eq:ewc_F}
    F = \frac{1}{N}\sum_{i=1}^N \nabla_\Theta \log p(x_{A,i}\:|\:\Theta_A^*)\:\nabla_\Theta \log p(x_{A_i}\:|\:\Theta_A^*)^T
\end{equation}
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{img/ewc.png}
		\caption{Schematizzazione del comportamento di EWC}
		\label{fig:ewc}
	\end{center}
\end{figure}
% TODO: dettagli implementativi?
% Implementazione: \cite{moriarty_ewc}
\subsection{Learning Without Forgetting}
L'approccio Learning Without Forgetting, delineato nell'omonima pubblicazione$^{\cite{li2017learning}}$, sfrutta esclusivamente i nuovi dati per addestrare il modello mantenendo la conoscenza già appresa.\\
Questo metodo parte da una rete che possiede parametri $\Theta$, di cui i parametri $\Theta_s$ sono condivisi fra i task e i parametri $\Theta_A$ sono specifici al task $A$. L'obiettivo è aggiungere parametri $\Theta_B$ per il nuovo task usando solo i nuovi dati senza deteriorare la conoscenza precedentemente appresa.\\\\
LWF inizia applicando il modello addestrato ai dati nuovi appena arrivati, registrando i risultati che esso produce. Dopodiché si addestra il modello, usando come loss la somma di due loss così definite:
\begin{equation}\label{eq:lwf_lossnew}
    loss_{new}(y_n, \overline{y}_n) = -y_n\cdot\log\overline{y}_n
\end{equation}
\begin{equation}\label{eq:lwf_lossnew}
    loss_{old}(y_o, \overline{y}_o) = -\sum_{i=1}^l y_o^{'(i)}\log y_o^{'(i)}
\end{equation}
dove
\begin{equation}\label{eq:lwf_lossnew}
    y_o^{'(i)} = \frac{\left(y_o^{(i)}\right)^{1/T}}{\sum_j \left(y_o^{(j)}\right)^{1/T}},\:\:\:\:\:
    \overline{y}_o^{'(i)} = \frac{\left(\overline{y}_o^{(i)}\right)^{1/T}}{\sum_j \left(\overline{y}_o^{(j)}\right)^{1/T}}
\end{equation}
$T$ è un parametro della \textit{Knowledge Distillation loss} settato a $T = 2$, $l$ è il numero di etichette, $y_n$ le etichette dei nuovi dati, $\overline{y}_n$ le predizioni dei nuovi dati, $y_o$ le precedenti previsioni del precedente modello e $\overline{y}_o$ le nuove previsioni del precedente modello.\\
La nuova loss è quindi
\begin{equation}\label{eq:lwf_loss}
    \lambda_o\cdot loss_{old}(Y_o, \overline{Y}_o) + loss_{new}(Y_n, \overline{Y}_n) + R(\Theta_s, \Theta_o, \Theta_n)
\end{equation}
Dove $\lambda_o$ è un peso che è tanto più grande quanto più sono importanti i vecchi dati rispetto ai nuovi, e $R$ è una funzione di regolarizzazione dei parametri.\\\\
Questo approccio, che tiene di conto della performance che il modello aveva fino all'arrivo dei nuovi dati, tende quindi a mantenere la rete neurale simile a quella già addestrata, evitando così di variare considerevolmente i pesi distanziandoli da quelli appresi sui dati precedenti.
% TODO: dettagli implementativi?

\section{Esperimenti}
\subsection{Overview} 
Gli esperimenti sono stati eseguiti su un comune personal computer, montante come sistema operativo Windows 10 in versione developer build 21390.2025 e all'interno della Windows Subsystem for Linux, con supporto alla GPU.\\\\% Specificare hardware?
Le \textit{callback} utilizzate durante le sessioni di addestramento sono state le seguenti:
\begin{itemize}
    \item[-] \textit{TensorBoard}, per poter visualizzare in tempo reale l'andamento dell'addestramento delle varie reti.
    \lstinputlisting[style=myPython, firstnumber=61, firstline=61, lastline=61]{code/wesad_totaltrain.py}
    \item[-] \textit{EarlyStopping}, per poter fermare anticipatamente l'addestramento in caso di diverse epoche svolte senza ottenere risultati migliori. La fermata anticipata è stata impostata sulla loss calcolata sul validation set, con pazienza di 10 epoche e ripristino dei migliori pesi trovati della rete neurale.
    \lstinputlisting[style=myPython, firstnumber=60, firstline=60, lastline=60]{code/wesad_totaltrain.py}
\end{itemize}
Tutti gli esperimenti sono stati eseguiti con batch di 256 elementi e per una durata massima 100 epoche, a meno di fermata anticipata. Inoltre il training set è sempre stato diviso in 75\% training set e 25\% validation set.
\lstinputlisting[style=myPython, firstnumber=63, firstline=63, lastline=63]{code/wesad_totaltrain.py}
\subsection{Model Selection}
La selezione del modello per ogni dataset è stata fatta in base all'accuratezza raggiunta durante il training offline. L'approccio alla model selection è stato il medesimo per entrambi i dataset, delineato di seguito:
\begin{enumerate}
    \item Selezione dei parametri dell'ottimizzatore\\
    L'algoritmo di ottimizzazione selezionato è Adam, poiché consigliato per problemi con molti dati rumorosi, efficiente in termini di tempo e spazio in memoria e facilità di fine-tuning degli iperparametri$^{\cite{adampaper}}$.\\
    Lo spazio dei parametri preso in considerazione è stato:
    \begin{itemize}
        \item[-] \textit{Learning rate} $\in \{0.001, 0.005, 0.008, 0.01\}$
        \item[-] $\beta_1 \in \{0.8, 0.85, 0.9, 0.95, 0.99\}$
        \item[-] $\beta_2 \in \{0.98, 0.985, 0.99, 0.995, 0.999\}$
    \end{itemize}
    \item Selezione dei parametri di regolarizzazione\\
    Il regolarizzatore L1L2 è stato applicato sul kernel, sui bias e sull'output del layer di output.\\
    I parametri presi in considerazione sono:
    \begin{itemize}
        \item[-] Kernel $\in \{0.0001, 0.00001, 0.000001\}$
        \item[-] Bias $\in \{0.0001, 0.00001, 0.000001\}$
        \item[-] Output $\in \{0.0001, 0.00001, 0.000001\}$
    \end{itemize}
    \item Selezione della struttura della rete neurale\\
    La rete neurale è stata scelta in base alla migliore performance ottenuta dopo 5 epoche di apprendimento.\\
    Le configurazioni prese in considerazione sono:
    \begin{itemize}
        \item[-] Layer $\in \{1, 2, 3, 4\}$
        \item[-] Unità $\in [10, 40]$
    \end{itemize}
\end{enumerate}
\pagebreak
Questo procedimento ha prodotto i seguenti modelli, uno per dataset:
\begin{itemize}
    \item[-] Dataset \textbf{WESAD}\\
    Due layer GRU da 18 unità con i seguenti parametri:
    \begin{itemize}
        \item[-] \textit{Learning rate} $= 0.005$
        \item[-] $\beta_1 = 0.99$
        \item[-] $\beta_2 = 0.99$
        \item[-] Kernel, bias, output $= 0.0001$
    \end{itemize}
    Il modello è capace di raggiungere in modo consistente il 99\% di accuratezza sul dataset durante l'addestramento offline. Questo lo rende un ottimo candidato per giudicare l'applicabilità dello stesso modello su diversi approcci di continual learning.
    \item[-] Dataset \textbf{ASCERTAIN}\\
    Due layer GRU da 24 unità con i seguenti parametri:
    \begin{itemize}
        \item[-] \textit{Learning rate} $= 0.01$
        \item[-] $\beta_1 = 0.9$
        \item[-] $\beta_2 = 0.99$
        \item[-] Kernel, bias, output $= 0.00001$
    \end{itemize}
    Anche questo modello è stato selezionato in base all'accuratezza sul dataset, che si attesta intorno al 45\%.
\end{itemize}
La scelta dei modelli si è basata sui risultati ottenuti nell'addestramento offline poiché è interessante comparare gli approcci che considerano i dati come disponibili un po' per volta, gli approcci continual, al caso in cui i dati sono resi disponibili fin da subito. Questo consente di verificare il cambiamento di prestazioni del solito modello nei diversi casi.
\subsection{Percentuale di replay}
La percentuale di dati da ripetere nella sessione di addestramento successiva, secondo lo scenario di replay, è stata scelta provando diverse percentuali in base all'accuratezza e alle altre metriche misurate nel continual training su dataset WESAD, con i risultati presentati nella tabella \ref{tab:replayperctest}.\\
\begin{table}[h]
    \begin{center}
        \begin{tabular}{l|c|c|c|c}
            \textbf{Percentuale} & \textbf{Accuratezza} & \textbf{ACC} & \textbf{BWT} & \textbf{FWT}\\
            \hline
            $10\%$ & 74,55\% & 0,7589 & 0,007 & 0,5539\\
            $15\%$ & 75,94\% & 0,7593 & -0,005 & 0,4693\\
            $20\%$ & 74,50\% & 0,7568 & 0,0054 & 0,3342\\
            \textbf{25\%} & \textbf{74,83\%} & \textbf{0,7707} & \textbf{0,0158} & \textbf{0,5275}\\
            $33\%$ & 74,71\% & 0,7823 & 0,0105 & 0,5948\\
        \end{tabular}
        \caption{Comparazione fra diverse percentuali di replay}
        \label{tab:replayperctest}
    \end{center}
\end{table}
La scelta è ricaduta sul 25\% di replay poiché le metriche misurate sono mediamente più alte delle altre percentuali, in particolare nel trasferimento di conoscenza. La scelta è anche influenzata dall'utilizzo di memoria, e un valore maggiore porta a modelli migliori pur usando più memoria durante l'addestramento.
% TODO grafico?
\pagebreak
\subsection{Valore di $m$}
Per quanto riguarda lo scenario episodic, l'iperparametro $m$ è stato scelto provando diversi valori sulla base dell'accuratezza e delle altre metriche misurate nell'episodic training su dataset WESAD, ottenendo i risultati elencati nella tabella \ref{tab:episodicmtest}\\\\
\begin{table}[h]
    \begin{center}
        \begin{tabular}{l|c|c|c|c}
            \textbf{$m =$} & \textbf{Accuratezza} & \textbf{ACC} & \textbf{BWT} & \textbf{FWT}\\
            \hline
            $40$ & 79,07\% & 0,8450 & 0,0516 & 0,3752\\
            $50$ & 78,08\% & 0,8082 & 0,0253 & 0,5651\\
            $60$ & 79,50\% & 0,8979 & 0,0998 & 0,6109\\
            \textbf{70} & \textbf{81,11\%} & \textbf{0,8589} & \textbf{0,0447} & \textbf{0,6351}\\
            $80$ & 80,83\% & 0,9064 & 0,0957 & 0,4223\\
            $90$ & 80,92\% & 0,9031 & 0,0892 & 0,4793
        \end{tabular}
        \caption{Comparazione fra vari valori di $m$ nell'addestramento episodico}
        \label{tab:episodicmtest}
    \end{center}
\end{table}

Viene scelto $m = 70$ poiché raggiunge un'accuratezza media superiore e un'accettabile trasferimento di conoscenza. Anche qua la scelta è dettata dall'utilizzo di memoria, e come dimostrato dai dati un valore maggiore di $m$ può risultare in generale preferibile.
% TODO grafico?

\section{Conclusione}
La scelta degli approcci adottati, elemento centrale di questo progetto, si è rivelata particolarmente laboriosa. Anzitutto si è reso necessario inquadrare precisamente lo scopo del lavoro, cioè la comparazione fra diverse tecniche di continual learning applicate a dati relativi allo human state monitoring. Questo ha richiesto la selezione di dataset appropriati e in seguito la scelta di semplici approcci di continual learning che potessero essere comparati. La comparazione ha quindi richiesto la ricerca e la scelta di metriche che ne misurassero i diversi aspetti di trasferimento della conoscenza e dell'accuratezza media e finale, oltre che i tempi di addestramento e la memoria utilizzata.\\
Il tutto è stato preceduto dalla ricerca e dallo studio personale dell'argomento, oltre che dalla scelta del linguaggio e delle librerie di sviluppo. Questo ha portato ad una lunga serie di esperimenti esplorativi, per poter studiare e testare i vari aspetti del machine learning e delle reti neurali prima, e del continual learning successivamente. Questi esperimenti hanno permesso di affrontare le diverse problematiche comuni ad un problema di machine learning: il preprocessing dei dati, l'individuazione degli iperparametri adatti ad un modello, la corretta progettazione di una rete neurale, i tempi di addestramento e altri ancora.\\\\
Tutto questo lavoro è confluito in una serie di esperimenti finali che hanno prodotto i risultati presentati nel prossimo capitolo.

% Aggiungere queste parti? \/
% Esperimenti
% Magari racconto velocemente i diversi approcci e difficoltà errori riscontrati?
% dovrei parlare di tutti i tentativi ed esperimenti da giugno a ora o solamente degli approcci "finali" adottati?
% inizialmente: studio su come realizzare le reti neurali, esperimenti personali per familiarizzare con API
% poi inizio studio su come affrontare il training...

% --> capitolo successivo, Risultati