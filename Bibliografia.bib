@misc{disarlidatasetpreproc,
    author = "Daniele Di Sarli",
    title = "RNN benchmarking multi-layer",
    url  = "https://colab.research.google.com/drive/1ko5kUsAkI4bDhW9EKH7WescO9rjJ71vp",
    addendum = "(accessed: 06.08.2021)",
    keywords = "python,dataset,preprocessing"
}

@misc{ewcpython,
    author = "Sean Morarity",
    title = "Continual Learning with Elastic Weight Consolidation in TensorFlow 2",
    url  = "https://seanmoriarity.com/2020/10/18/continual-learning-with-ewc/",
    addendum = "(accessed: 10.07.2021)",
    keywords = "python,ewc,code"
}

@misc{teaching2020,
    author = "TEACHING",
    title = "Toolkit for building Efficient Autonomous appliCations leveraging Humanistic INtelliGence",
    url  = "https://www.teaching-h2020.eu/",
    addendum = "(accessed: 06.08.2021)",
    keywords = "project"
}

@misc{lomonaco_2019,
    title={Continual learning for production systems}, url={https://medium.com/continual-ai/continual-learning-for-production-systems-304cc9f60603},
    journal={Medium},
    publisher={ContinualAI},
    author={Lomonaco, Vincenzo},
    year={2019},
    month={Aug}
} 

@article{DBLP:journals/corr/LiH16e,
  author    = {Zhizhong Li and
               Derek Hoiem},
  title     = {Learning without Forgetting},
  journal   = {CoRR},
  volume    = {abs/1606.09282},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.09282},
  archivePrefix = {arXiv},
  eprint    = {1606.09282},
  timestamp = {Thu, 31 Dec 2020 11:34:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/LiH16e.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Lopez-PazR17,
  author    = {David Lopez{-}Paz and
               Marc'Aurelio Ranzato},
  title     = {Gradient Episodic Memory for Continuum Learning},
  journal   = {CoRR},
  volume    = {abs/1706.08840},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.08840},
  archivePrefix = {arXiv},
  eprint    = {1706.08840},
  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Lopez-PazR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v15-glorot11a,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {
http://proceedings.mlr.press/v15/glorot11a.html
},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@incollection{MCCLOSKEY1989109,
title = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {24},
pages = {109-165},
year = {1989},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60536-8},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605368},
author = {Michael McCloskey and Neal J. Cohen},
abstract = {Publisher Summary
Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.}
}

@TECHREPORT{Riedmiller92rprop-,
    author = {Martin Riedmiller and Heinrich Braun},
    title = {RPROP - A Fast Adaptive Learning Algorithm},
    institution = {Proc. of ISCIS VII), Universitat},
    year = {1992}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{diethe2019continual,
      title={Continual Learning in Practice}, 
      author={Tom Diethe and Tom Borchert and Eno Thereska and Borja Balle and Neil Lawrence},
      year={2019},
      eprint={1903.05202},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Jha_2021,
   title={Continual learning in sensor-based human activity recognition: An empirical benchmark analysis},
   volume={575},
   ISSN={0020-0255},
   url={http://dx.doi.org/10.1016/j.ins.2021.04.062},
   DOI={10.1016/j.ins.2021.04.062},
   journal={Information Sciences},
   publisher={Elsevier BV},
   author={Jha, Saurav and Schiemer, Martin and Zambonelli, Franco and Ye, Juan},
   year={2021},
   month={Oct},
   pages={1–21}
}

@misc{priyanshu2021continual,
      title={Continual Distributed Learning for Crisis Management}, 
      author={Aman Priyanshu and Mudit Sinha and Shreyans Mehta},
      year={2021},
      eprint={2104.12876},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agilemanifesto,
    author = "Kent Beck; James Grenning; Robert C. Martin; Mike Beedle; Jim Highsmith; Steve Mellor; Arie van Bennekum; Andrew Hunt; Ken Schwaber; Alistair Cockburn; Ron Jeffries; Jeff Sutherland; Ward Cunningham; Jon Kern; Dave Thomas; Martin Fowler; Brian Marick",
    title = "Manifesto for Agile Software Development",
    url  = "http://agilemanifesto.org/",
    addendum = "(accessed: 06.08.2021)",
    keywords = "project"
}